{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b10def-f13d-45a0-80b8-47cfc99afe53",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Lösungen zu den Zusatzübungen zum Notebook \"Datenanalyse\"\n",
    "\n",
    "☝️ Beachte 1: es gibt beim Programmieren fast immer verschiedene Lösungswege. Deine Lösung mag anders aussehen, aber dennoch zum gewünschten Resultat führen. Das richtige Resultat ist das Wichtigste. \n",
    "\n",
    "☝️ Beachte 2:  In diesem Notebook arbeiten wir größtenteils mit einer Datei, die Du in Übung 1 selbst herunterladen musst. Die meisten Lösungen lassen sich folglich erst ausführen, wenn Du den korrekten Pfad zur heruntergeladenen Datei bei `path` eingesetzt hast. \n",
    "\n",
    "☝️ Beachte 3:  Anders als im siebten Notebook kommt hier auch die dot-Notation für den Spaltenzugriff (`df.column` statt `df[\"column\"]`) zum Einsatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ce585-0594-4e02-af43-73815d7ff215",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1. Lade Dir den Datensatz mit 1 Million Sätzen aus deutschsprachigen Nachrichtentexten (\"News\") aus dem Jahr 2022 von der Seite des Projekts [Wortschatz Leipzig](https://wortschatz.uni-leipzig.de/de/download/German) herunter und speichere ihn an einem sinnvollen Ort. Entpacke die Datei (falls das bei Windows auf Anhieb nicht funktioniert, empfiehlt sich das Programm [WinRAR](https://www.winrar.de)) und lies die Datei \"deu_news_2022_1M-sentences.txt\" mithilfe von pandas ein. Das DataFrame soll `news_df` heißen und aus zwei Spalten (`sentence_id` und `sentence`) sowie 1 Million Zeilen bestehen. Spaltennamen kannst Du mithilfe des `names`-Argument beim Einlesen definieren. Lass Dir die ersten zehn Zeilen ausgeben.\n",
    "  \n",
    "   Befinden sich wirklich 1 Million Sätze im DataFrame? Falls dies bei Deinem DataFrame nicht der Fall ist, überleg Dir, was schief gelaufen sein könnte. Denn in der Datei \"deu_news_2022_1M-sentences.txt\" befinden garantiert 1 Million Sätze.\n",
    "\n",
    "   💡 Tipp: Zeilen, die ein öffnendes, aber kein schließendes Anführungszeichen beinhalten, führen dazu, dass pandas \"\\t\" am Ende der Zeile literal anstatt als Trennzeichen interpretiert. Dadurch landen mehrere Sätze in *einer* Zeile (nämlich alle bis zum nächsten \"schließenden\" Anführungszeichen), weswegen schlussendlich weniger als 1 Million Zeilen im DataFrame stehen. Informiere Dich in der Dokumentation von pandas über den Parameter `quoting`, mit dessen Hilfe Du dieses Verhalten übersteuern kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e091c30-60f2-4094-8839-ff6a30c63852",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"Einlesen der Daten, Spezifizieren der Spaltennamen, sowie Steuern des Verhaltens bei Anführungszeichen:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mDer Parameter 'quoting' löst das Problem, dass mehr als ein Satz in einer Zeile landet. Das Problem entsteht bei Sätzen,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mdie ein öffnendes Anführungszeichen beinhalten, aber kein schließendes. pandas interpretiert dadurch die folgenden Trennzeichen '\\t'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mAnstatt einer ominösen 3, kannst Du auch zusätzlich das Modul csv importieren und quoting=csv.QUOTE_NONE spezifizieren,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mdie beiden Optionen sind gleichbedeutend.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m news_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(news_df\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m#Ausgabe der Anzahl an Spalten und Zeilen des DataFrames: das DataFrame sollte 1 Million Zeilen umfassen\u001b[39;00m\n\u001b[1;32m     16\u001b[0m news_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m#Ausgabe der ersten zehn Zeilen\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "import pandas as pd #Importieren des Moduls\n",
    "\n",
    "#Spezifizieren des Pfades zur einzulesenden Datei (hier leer, da individueller Speicherort)\n",
    "path = \"\"\n",
    "\n",
    "\"\"\"Einlesen der Daten, Spezifizieren der Spaltennamen, sowie Steuern des Verhaltens bei Anführungszeichen:\n",
    "Der Parameter 'quoting' löst das Problem, dass mehr als ein Satz in einer Zeile landet. Das Problem entsteht bei Sätzen,\n",
    "die ein öffnendes Anführungszeichen beinhalten, aber kein schließendes. pandas interpretiert dadurch die folgenden Trennzeichen '\\t'\n",
    "literal anstatt als Trennzeichen, bis zum nächsten Anführungszeichen. Indem wir quoting=3 spezifizieren, teilen wir pandas mit,\n",
    "dass KEINE Anführungszeichen beim Einlesen als literal interpretiert werden sollen, vgl. https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html;\n",
    "Anstatt einer ominösen 3, kannst Du auch zusätzlich das Modul csv importieren und quoting=csv.QUOTE_NONE spezifizieren,\n",
    "die beiden Optionen sind gleichbedeutend.\"\"\"\n",
    "news_df = pd.read_csv(path, sep=\"\\t\", encoding=\"utf-8\", names=[\"sentence_id\", \"sentence\"], quoting=3)\n",
    "\n",
    "print(news_df.shape) #Ausgabe der Anzahl an Spalten und Zeilen des DataFrames: das DataFrame sollte 1 Million Zeilen umfassen\n",
    "news_df.head(10) #Ausgabe der ersten zehn Zeilen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa273c4-bae0-403b-b4b8-b1648fab4f83",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. Finde heraus, ob es in den deutschsprachigen Nachrichten 2022 häufiger um die Ukraine oder Corona ging (unter der Annahme, dass der Datensatz repräsentativ für die deutschsprachigen Nachrichten ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8feff41-6ac4-4134-bd8c-16d1d793cc9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ukraine \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m[news_df\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUkraine\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;66;03m#Filtern des DataFrames nach dem Vorkommen von \"Ukraine\" in der Spalte \"sentence\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m corona \u001b[38;5;241m=\u001b[39m news_df[news_df\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorona\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;66;03m#Filtern des DataFrames nach dem Vorkommen von \"Corona\" in der Spalte \"sentence\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Ausgabe der Länge der beiden Sub-DataFrames\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "ukraine = news_df[news_df.sentence.str.contains(\"Ukraine\")] #Filtern des DataFrames nach dem Vorkommen von \"Ukraine\" in der Spalte \"sentence\"\n",
    "corona = news_df[news_df.sentence.str.contains(\"Corona\")] #Filtern des DataFrames nach dem Vorkommen von \"Corona\" in der Spalte \"sentence\"\n",
    "\n",
    "#Ausgabe der Länge der beiden Sub-DataFrames\n",
    "print(\"Sätze mit 'Ukraine':\", len(ukraine))\n",
    "print(\"Sätze mit 'Corona':\", len(corona))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c511c-69f5-49be-99ea-f00e41480e6e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Schaffe eine zusätzliche Spalte im DataFrame, in der die jeweilige Länge des Satzes in Wörtern verzeichnet ist.\n",
    "\n",
    "   Wie lange ist der längste Satz? Wie lange ist der kürzeste Satz? Und was ist die durchschnittliche Satzlänge?\n",
    "\n",
    "   Lass Dir den längsten Satz sowie die kürzesten Sätze ausgeben!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ed1b7d-e510-4edc-bb6e-d9cd7fc8c68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m news_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m#Zählen der Leerschläge und Addieren von 1, um Anzahl Wörter pro Satz auszurechnen\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(news_df\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39mmax(), news_df\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39mmin(), news_df\u001b[38;5;241m.\u001b[39mlength\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;66;03m#Ausgabe des maximalen, minimalen und durchschnittlichen Werts in der Spalte \"length\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Filtern des DataFrames danach, dass in der Spale \"length\" der maximale bzw. minmale Wert verzeichnet steht, Ausgabe der Werte mithilfe der 'values'-Methode (um nur die Werte zu erhalten)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "news_df[\"length\"] = news_df.sentence.str.count(\" \") + 1 #Zählen der Leerschläge und Addieren von 1, um Anzahl Wörter pro Satz auszurechnen\n",
    "print(news_df.length.max(), news_df.length.min(), news_df.length.mean()) #Ausgabe des maximalen, minimalen und durchschnittlichen Werts in der Spalte \"length\"\n",
    "\n",
    "#Filtern des DataFrames danach, dass in der Spale \"length\" der maximale bzw. minmale Wert verzeichnet steht, Ausgabe der Werte mithilfe der 'values'-Methode (um nur die Werte zu erhalten)\n",
    "print(news_df[news_df.length == news_df.length.max()].sentence.values)\n",
    "print(news_df[news_df.length == news_df.length.min()].sentence.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc9ac6-3d4d-4670-91a4-afd87c50979b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "4. Sortiere das DataFrame nach der Länge der Sätze in absteigender Reihenfolge. Welcher Schritt ist anschließend noch sinnvoll?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b4a8c10-12d9-45da-b352-3c46065209e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Sortieren des DataFrames nach den Werten der Spalte \"length\" sowie sinnvollerweise Zurücksetzen der Indizes inkl. \"droppen\" der alten Indizes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m news_df \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m news_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Sortieren des DataFrames nach den Werten der Spalte \"length\" sowie sinnvollerweise Zurücksetzen der Indizes inkl. \"droppen\" der alten Indizes\n",
    "news_df = news_df.sort_values(\"length\", ascending=False).reset_index(drop=True)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac5d7a8-58c4-4223-8d50-9666ef2ee2f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "5. Bereinige sämtliche Sätze derart, dass Sonderzeichen von allen Wortanfängen und -enden entfernt werden. Dazu steht Dir die Liste `special_signs` zur Verfügung. Groß- und Kleinschreibung sollst Du beibehalten. Jeder Satz soll auch nach dem Preprocessing als ein string vorliegen. \n",
    "\n",
    "    💡 Tipp: Verwende die pandas-eigene, vektorisierte Art der Datenbearbeitung und denk daran, dass Du diese auch in Kombination mit selbst definierten Funktionen verwenden kannst.  \n",
    "    \n",
    "    📌 Herausforderung: Verwende maximal eine Zeile zur Bereinigung der Sätze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a518968f-e981-43d7-9126-9d895b0c667d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(preprocessed_words) \u001b[38;5;66;03m#Rückgabe eines wieder als string zusammengesetzten Satzes mit bereinigten Wörtern\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Überschreiben der Spalte \"sentence\"\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m news_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mapply(preprocessing) \u001b[38;5;66;03m#Anwenden der benutzerdefinierten Funktion auf alle Zeilen in der Spalte \"sentence\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Herausforderung\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#news_df[\"sentence\"] = news_df.sentence.apply(lambda sentence: \" \".join([word.strip(specials_signs_str) for word in sentence.split(\" \") if len(word.strip(specials_signs_str)) > 0]))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m news_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "special_signs = ['‰', ':', '§', '´', '.', '́', ';', '❓', '‑', '”', ')', ',', '<', '″', '»', '−', '✔', '•', '\"', '`', '〉', '†', '*', '>', '&', \"'\", '‹', '/', '‚', '®', '°', '‒', '▶', '(', '%', '‘', '€', '«', 'Ł', '═', '„', '!', '–', '?', '-', '︎', '—', '“', '·', '…', '‟', '‡','’', '$', 'ł', '~', '™', '›', '+']\n",
    "specials_signs_str = \"\".join(special_signs) #Casten in string, da strip-Methode einen string mit zu strippenden Zeichen erwartet\n",
    "\n",
    "#Definieren einer eigenen Funktion, die unten auf jeden Satz angewandt wird\n",
    "def preprocessing(sentence):\n",
    "    words = sentence.split(\" \") #Splitten in Wörter\n",
    "    #Bereinigen der Wörter (leere strings werden durch if-Bedingung übersprungen)\n",
    "    preprocessed_words = [word.strip(specials_signs_str) for word in words if len(word.strip(specials_signs_str)) > 0] \n",
    "    return \" \".join(preprocessed_words) #Rückgabe eines wieder als string zusammengesetzten Satzes mit bereinigten Wörtern\n",
    "\n",
    "#Überschreiben der Spalte \"sentence\"\n",
    "news_df[\"sentence\"] = news_df.sentence.apply(preprocessing) #Anwenden der benutzerdefinierten Funktion auf alle Zeilen in der Spalte \"sentence\"\n",
    "\n",
    "#Herausforderung\n",
    "#news_df[\"sentence\"] = news_df.sentence.apply(lambda sentence: \" \".join([word.strip(specials_signs_str) for word in sentence.split(\" \") if len(word.strip(specials_signs_str)) > 0]))\n",
    "\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef371d-c6ca-40a4-9bcd-db2a9ea0dcaf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "6. Mit welchen zehn Wörtern beginnen die Nachrichtensätze am häufigsten?\n",
    "\n",
    "   💡 Tipp: Um jeweils auf das n-te Element einer Liste in jeder Zeile einer bestimmten Spalte zuzugreifen, kannst Du `.str[n]` an den Spaltennamen anhängen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d42cda-3359-4e8a-b780-d10ae6d348f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Splitten der Sätze in Wörter, Indizieren des ersten Elements, Berechnen der Häufigkeitsverteilung und Ausgabe der obersten zehn Elemente\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Splitten der Sätze in Wörter, Indizieren des ersten Elements, Berechnen der Häufigkeitsverteilung und Ausgabe der obersten zehn Elemente\n",
    "news_df.sentence.str.split(\" \").str[0].value_counts().head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538e31a-d319-4178-bf95-9ddf90b83db0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "7. Und mit welchen zehn Wörtern enden die die Nachrichtensätze am häufigsten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe75fa4-a64f-4919-8b67-62e655f476f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Splitten der Sätze in Wörter, Indizieren des letzten Elements, Berechnen der Häufigkeitsverteilung und Ausgabe der obersten zehn Elemente\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39msentence\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Splitten der Sätze in Wörter, Indizieren des letzten Elements, Berechnen der Häufigkeitsverteilung und Ausgabe der obersten zehn Elemente\n",
    "news_df.sentence.str.split(\" \").str[-1].value_counts().head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1472b-d141-4d9e-8de3-856cf46aca02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "8. Wortschatz Leipzig stellt nicht nur eine Million Sätze zur Verfügung, sondern listet auch auf, wann und von welchem Nachrichtenportal die Sätze extrahiert wurden. Diese Informationen sind jedoch in zwei weiteren Dateien gespeichert, die Du ebenfalls bereits heruntergeladen und entpackt hast. Sämtliche Quellen sowie Daten (wann wurde der Satz heruntergeladen?) sind in \"deu_news_2022_1M-sources.txt\" aufgelistet und mit Quellen-IDs versehen, die Zuordnung zwischen Quellen-ID und Satz-ID findet sich wiederum in \"deu_news_2022_1M-inv_so.txt\".\n",
    "\n",
    "    Deine Aufgabe ist es nun, diese Informationen (Quelle und Datum) mit dem DataFrame `news_df` zu vereinen. Dazu bietet sich eine Dir vermutlich noch unbekannte Methode namens `merge` an. Informiere Dich [hier](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) über die Methode. Bei `merge` kannst Du mithilfe des `on`-Parameters spezifizieren, auf Basis der Werte welcher Spalte die Zusammenführung zweier DataFrames erfolgen soll. Wichtig ist dabei, dass der angegebene Spaltenname in beiden DataFrames existiert. Stelle sicher, dass du am Ende immer noch eine Million Sätze in Deinem DataFrame hast, sowie, dass keine fehlenden Werte (die etwa aufgrund eines falschen Mergings eingetragen wurden) darin vorkommen. Verwende dazu entweder `if`-Bedingungen, oder, eleganter, `assert`-Statements (mehr dazu [hier](https://realpython.com/python-assert-statement/#getting-to-know-assertions-in-python)). \n",
    "    \n",
    "    Überprüfe abschließend bei ein paar Quellenlinks, ob die jeweiligen Sätze tatsächlich auf der entsprechenden Website zu finden sind.\n",
    "    \n",
    "    ⚠️ Achtung: Wenn Du mehrfach hintereinander dieselben DataFrames zusammenfügst, riskierst Du einen `KeyError` bei der als `on`-Parameter übergebenen Spalte. Die Fehlermeldung rührt daher, dass pandas bei mehrmaligem Mergen bereits existierende Spaltennamen um ein Suffix ergänzt, da Spaltennamen einzigartig sein müssen. Schaffe deshalb als erstes eine Kopie von `news_df` mithilfe von `news_df_enriched = news_df.copy()` und arbeite fortan mit `news_df_enriched`. Jedes Mal wenn Du die ganze Zelle ausführst, um die DataFrames zu mergen, passiert dies somit auf Basis einer (immer wieder) neu erstellten Kopie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04e085e-4256-4c0b-b9a2-585ac5418a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m news_df_enriched \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Spezifizieren der Pfade zur einzulesenden Datei (hier leer, da individueller Speicherort)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sources \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m], encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "news_df_enriched = news_df.copy()\n",
    "\n",
    "#Spezifizieren der Pfade zur einzulesenden Datei (hier leer, da individueller Speicherort)\n",
    "sources = pd.read_csv(\"\", sep=\"\\t\", quoting=3, names=[\"source_id\", \"source\", \"date\"], encoding=\"utf-8\")\n",
    "mapping = pd.read_csv(\"\", sep=\"\\t\", quoting=3, names=[\"source_id\", \"sentence_id\"], encoding=\"utf-8\")\n",
    "\n",
    "news_df_enriched = news_df_enriched.merge(mapping, on=\"sentence_id\") #Zusammenführen von news_df und mapping, basierend auf Werten der Spalte \"sentence_id\"\n",
    "news_df_enriched = news_df_enriched.merge(sources, on=\"source_id\") #Zusammenführen von news_df (jetzt mit source_id aus vorangegangenem Merging) und sources, basierend auf Werten der Spalte \"source_id\"\n",
    "\n",
    "#Überprüfen, ob Länge von news_df immer noch 1 Million mithilfe des assert-Statements\n",
    "assert len(news_df_enriched) == 1000000\n",
    "\n",
    "\"\"\"Überprüfen, ob fehlende Werte in news_df vorkommen mithilfe von isna(), das ein DataFrame mit True/False für jede\n",
    "einzelne Zelle ausgibt. False ist bei Python gleichbedeutung mit 0 (True mit 1), weswegen wir die sum-Methode anhängen können,\n",
    "die standardmäßig spaltenweise aufsummiert, wobei immer noch jede Spalte Null ergeben sollte. Anschließend können wir auf die\n",
    "resultierende Series nochmal sum() anwenden, um die Spaltensummen zusammenzuzählen. Ergibt dies immer noch Null, finden sich\n",
    "keinerlei fehlende Werte in news_df_enriched. Lasse Dir ggf. Zwischenschritte ausgeben, um das Ergebnis nachvollziehen zu können\"\"\"\n",
    "assert news_df_enriched.isna().sum().sum() == 0\n",
    "\n",
    "#Ausgabe von ein paar Links zur manuellen Überprüfung, ob Sätze tatsächlich aus der jeweiligen Quelle stammen\n",
    "some_indices = [330, 8374, 99473]\n",
    "for index in some_indices:\n",
    "    print(news_df_enriched.loc[index, \"sentence\"], \"sollte von\", news_df_enriched.loc[index, \"source\"], \"stammen. Stimmts?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60e5cd9-21ff-40d0-b50f-41b2e47e07e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "9. Füge eine weitere Spalte namens `country` hinzu, die basierend auf der Top-Level-Domain dem Quellenlink (z.B. \".de\" oder \".ch\") angibt, aus welchem Land der jeweilige Nachrichtensatz stammt. Dies lässt sich am besten mithilfe eines regulären Ausdrucks (vgl. sechstes Notebook) lösen. Pandas wiederum stellt mit `findall` eine nützliche string-Methode zur Verfügung, die wir wie gewohnt auf eine ganze Spalte anwenden können und der wir problemlos einen regulären Ausdruck übergeben können. \n",
    "\n",
    "    📌 Herausforderung: Lass Dir eine schön formatierte Übersicht über die absolute und eine relative Häufigkeitsverteilung der Länder ausgeben. Folgender Screenshot ist eine Idee für die Formatierung, Du kannst es aber auch anders umsetzen.\n",
    "    \n",
    "    <img src=\"../../3_Dateien/Grafiken_und_Videos/tld_overview.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c089c63f-ebf9-476d-9b70-50758f623653",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df_enriched' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m news_df \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df_enriched\u001b[49m \u001b[38;5;66;03m#\"Zurückverweisen\" von news_df_enriched auf news_df, da dies der simplere Variablenname ist\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Definieren der RegEx: ein Punkt (escaped mit Backslash), gefolgt von zwei oder mehr kleingeschriebenen Buchstaben, gefolgt von Schrägstrich (escaped mit Backslash)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df_enriched' is not defined"
     ]
    }
   ],
   "source": [
    "news_df = news_df_enriched #\"Zurückverweisen\" von news_df_enriched auf news_df, da dies der simplere Variablenname ist\n",
    "\n",
    "import re\n",
    "\n",
    "#Definieren der RegEx: ein Punkt (escaped mit Backslash), gefolgt von zwei oder mehr kleingeschriebenen Buchstaben, gefolgt von Schrägstrich (escaped mit Backslash)\n",
    "pattern = r\"\\.[a-z]{2,}\\/\"\n",
    "\n",
    "\"\"\"Schaffen einer neuen Spalte, die jeweils das erste Element (Index 0) der durch findall(pattern) produzierten Liste \n",
    "mit Regex-Matches beinhält (bereinigt vom Punkt zu Beginn und dem Schrägstrich am Ende)\"\"\"\n",
    "news_df[\"country\"] = news_df.source.str.findall(pattern).str[0].str.strip(\"./\") \n",
    "\n",
    "#Herausfoderung\n",
    "#Schaffen zweier dictionaries mit den absoluten und relativen Häufigkeitsverteilungen (dictionaries eignen sich besser als Series für die Iteration unten)\n",
    "absolute, relative = dict(news_df.country.value_counts()), dict(news_df.country.value_counts(normalize=True))\n",
    "\n",
    "#Ausgabe einer Überschrift sowie eines horizontalen Trennbalkens, Formatierung mithilfe von f-strings\n",
    "print(f\"{'TLD':10}{'Absolute':>10}{'Relative':>10}\\n{'-'*30}\")\n",
    "\n",
    "#Iteration über eines der beiden dictionaries, Ausgabe des Schlüssels sowie der beiden Werte aus den dictionaries, Formatierung mithilfe von f-strings\n",
    "for key, val in absolute.items():\n",
    "    print(f\"{key:10}{val:10}{relative[key]:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54eb17e-445b-4936-821c-463fd9bf2fe8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "10. Schaffe ein Sub-DataFrame mit Nachrichtensätzen aus dem [DACH](https://de.wikipedia.org/wiki/D-A-CH)-Raum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db4c221-7629-4498-bff2-c4d8994bfe3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Schaffen eines Sub-DataFrames, indem news_df in der Spalte \"country\" nach den Werten \"de\", \"at\" und \"ch\" gefiltert wird\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dach_df \u001b[38;5;241m=\u001b[39m \u001b[43mnews_df\u001b[49m[news_df\u001b[38;5;241m.\u001b[39mcountry\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mch\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\n\u001b[1;32m      3\u001b[0m dach_df\u001b[38;5;241m.\u001b[39mhead() \u001b[38;5;66;03m#Ausgabe der obersten Zeilen des Sub-DataFrames\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Schaffen eines Sub-DataFrames, indem news_df in der Spalte \"country\" nach den Werten \"de\", \"at\" und \"ch\" gefiltert wird\n",
    "dach_df = news_df[news_df.country.isin([\"de\", \"at\", \"ch\"])]\n",
    "dach_df.head() #Ausgabe der obersten Zeilen des Sub-DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73e1b1-7db8-4e2d-a7c1-6dd1251124d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "11. Zum Schluss wollen wir noch überprüfen, ob das Sampling der Nachrichtensätze im Datensatz einigermaßen repräsentativ für die deutschsprachigen Länder ist, wobei wir uns der Einfachheit halber wieder auf den DACH-Raum beschränken. Deutschland ist ungefähr um den Faktor 10 größer als Österreich resp. die Schweiz (überprüfe gerne die aktuellen Einwohner:innenzahl der drei Länder). Entsprechend sollten zirka zehn mal so viele Nachrichtensätze aus Deutschland stammen als aus Österreich bzw. der Schweiz. Gleichzeitig sollte der Anteil an Nachrichtensätzen aus Deutschland, Österreich und der Schweiz aber auch einigermaßen gleichmäßig über das Jahr 2022 verteilt sein. Idealerweise wurden nicht bloß im Januar Schweizer Quellen, im Februar österreichische und den Rest des Jahres deutsche gesammelt... Die Verteilung nach Land über die Monate des Jahres 2022 hinweg wollen wir uns als Plot ausgeben lassen. Bevor Du diesen Plot erstellst, führe folgende Vorbereitungsschritte aus:\n",
    "\n",
    "    - Wenn Du dir die Daten (wann wurde der Satz heruntergeladen?) in `news_df` anschaust (z.B. mittels `news_df[~news_df.date.str.startswith(\"2022\")])`, siehst Du, dass einige unrealistische Daten dabei sind. Da hat wohl das Web-Scraping bzw. das Postprocessing versagt... Filtere das DataFrame derart, dass nur Sätze aus dem Jahr 2022 übrigbleiben.\n",
    "    - Da wir die Verteilung anstatt über 365 Tage vereinfacht über zwölf Monate hinweg plotten wollen, schaffe eine neue Spalte mit dem jeweiligen Extraktionsmonat des Nachrichtensatzes. Sortiere das DataFrame anschließend nach den Werten dieser neuen Spalte.\n",
    "    </br></br>\n",
    "    \n",
    "    Analysiere nun folgenden Plot und erstelle ihn anschließend selbst. Überleg Dir abschließend, ob das Sampling repräsentativ für die Größe der DACH-Länder über die Monate hinweg ist.\n",
    "    \n",
    "    \n",
    "    <img src=\"../../3_Dateien/Grafiken_und_Videos/news_dach.png\" width=\"600\"/> </br>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1458ecd-c0ec-4149-9f78-a9ca80983b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \u001b[38;5;66;03m#Importieren von matplotlib.pyplot\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dach_df \u001b[38;5;241m=\u001b[39m dach_df[dach_df\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;66;03m#Wegfiltern von Daten außerhalb des Jahres 2022\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dach_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dach_df\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#Schaffen einer neuen Spalte mit Monaten durch Splitten nach \"-\" und Indizieren des zweiten Elements\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #Importieren von matplotlib.pyplot\n",
    "\n",
    "dach_df = dach_df[dach_df.date.str.startswith(\"2022\")] #Wegfiltern von Daten außerhalb des Jahres 2022\n",
    "\n",
    "dach_df[\"month\"] = dach_df.date.str.split(\"-\").str[1] #Schaffen einer neuen Spalte mit Monaten durch Splitten nach \"-\" und Indizieren des zweiten Elements\n",
    "\n",
    "dach_df = dach_df.sort_values(\"month\", ascending=True) #Sortieren des DataFrames nach Monat\n",
    "\n",
    "tlds = [\"de\", \"at\", \"ch\"] #Definieren einer Liste mit DACH-TLDs\n",
    "\n",
    "#Iterieren über die TLDs\n",
    "for tld in tlds:\n",
    "    tld_df = dach_df[dach_df.country == tld] #Schaffen eines Sub-DataFrames mit Nachrichtensätzen nur aus dem jeweiligen Land\n",
    "    x = tld_df.month.unique() #Definieren der Werte, die auf der x-Achse geplottet werden (einzigartige Werte in der Spalte \"month\", d.h. die zwölf Monate)\n",
    "    \"\"\"Definieren der Werte, die auf der y-Achse geplottet werden, indem das Sub-DataFrame nach Werten in der Spalte \"month\" gruppiert wird und für jede Gruppe\n",
    "    die Anzahl an Zeilen über size() ermittelt wird. Der Wert jeden Monats wird anschließend durch die Gesamtanzahl an Zeilen im jeweiligen Monat in dach_df geteilt,\n",
    "    um den relativen Anteil des jeweiligen Landes im entsprechenden Monat auszurechnen.\"\"\"\n",
    "    y = tld_df.groupby([\"month\"]).size() / dach_df.groupby([\"month\"]).size()\n",
    "    plt.plot(x, y, 'o-') #Plotting im 'o-'-Stil\n",
    "\n",
    "#Plotten von Titel, Achsenbeschriftungen und Legende\n",
    "plt.title(\"Anzahl an Nachrichtensätzen aus den DACH-Ländern\")\n",
    "plt.xlabel(\"Monate 2022\")\n",
    "plt.ylabel(\"Anteil an DACH-Quellen/Monat\")\n",
    "plt.legend(tlds, loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3dfe-b087-4b29-aea3-0cedd06a9405",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Antwort auf abschließende Frage: Das Sampling ist definitiv gleichmäßig über die Monate hinweg. Der jeweilige Anteil der Länder entspricht einigermaßen ihrer Größenordnung, wobei der Anteil der Schweizer Quellen etwas zu niedrig und der Anteil der österreichischen Quellen etwas zu hoch ausfällt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}